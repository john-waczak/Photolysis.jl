[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mcm",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "quantum_yields.html",
    "href": "quantum_yields.html",
    "title": "Cross Sections, Quantum Yields, and Photolysis Rates",
    "section": "",
    "text": "Quantum Yields"
  },
  {
    "objectID": "cross_sections.html",
    "href": "cross_sections.html",
    "title": "Cross Sections, Quantum Yields, and Photolysis Rates",
    "section": "",
    "text": "Cross Sections"
  },
  {
    "objectID": "cross_sections/O3.html",
    "href": "cross_sections/O3.html",
    "title": "Cross Sections, Quantum Yields, and Photolysis Rates",
    "section": "",
    "text": "\\(\\mathrm{O_3}\\)\n\nusing Plots, MLPlotRecipes\nusing HDF5\nusing Tables, DataFrames, CSV\nusing MLJ, MLJGaussianProcesses\nusing StatsBase \n\n\ninclude(\"./utils.jl\")\n\ndata_to_df\n\n\n\nh5_path = \"../../data/photolysis_data.h5\" \nspecies = \"O3\" \n\nλs, σs, T1s, T2s, source_idx, T_units, category, formula, λ_units, σ_units = get_raw_data(h5_path, species)\n\nmin_data_λ = minimum(λs)\nmax_data_λ = maximum(λs)\n\nprintln(min_data_λ)\nprintln(max_data_λ)\n\n52.6\n1100.0\n\n\n\n# load in spectrometer wavelengths\nhr4000_df = CSV.File(\"../hr4000_wavelengths.txt\") |> DataFrame ; \n\n\nprintln(nrow(hr4000_df))\nprintln(maximum(hr4000_df.λ))\nprintln(minimum(hr4000_df.λ))\n\n3648\n1120.216\n194.249\n\n\n\nΔλ = 50 # nm for padding\ndf = data_to_df(λs, σs, T1s, T2s, source_idx; λ_lb=minimum(hr4000_df.λ)-Δλ, λ_ub=maximum(hr4000_df.λ)+Δλ)\nprintln(nrow(df))\ndescribe(df)\n\n366284\n\n\n\n4×7 DataFrameRowvariablemeanminmedianmaxnmissingeltypeSymbolFloat64RealFloat64RealInt64DataType1λ565.311144.91539.961100.00Float642σ9.07613e-193.4377e-258.0905e-221.41e-170Float643T293.791290.0293.0300.00Float644source_id130.2841175.02490Int64\n\n\n\nnskip = 5\n\np2 = scatter(\n    df.λ[1:nskip:end],\n    log10.(df.σ[1:nskip:end]),\n    zcolor=df.T[1:nskip:end],\n    ms=3, \n    msw=0,\n    xlabel=\"λ [$(λ_units)]\",\n    ylabel=\"log10(σ) [cm²]\",\n    alpha=0.7,\n    rightmargin=10Plots.mm,\n    colorbar_title=\"T [$(T_units)]\",\n    title=\"Absorption Cross Section for O₃\",\n    label=\"\",\n)\n\nsavefig(\"O3_p2.png\")\n\ndisplay(p2)\n\n\n\n\n\np1 = histogram(df.λ, xlabel=\"λ\", ylabel=\"counts\", label=\"\", margin=10Plots.mm)\np2 = histogram(df.T, xlabel=\"T\", label=\"\", margin=10Plots.mm)\np3 = histogram(log10.(df.σ), xlabel=\"log10(σ)\", label=\"\", margin=10Plots.mm)\n#p3 = histogram(df.σ, xlabel=\"σ\", ylabel=\"counts\")\n\nphist = plot(p1, p2, p3, layout=(1,3), size=(1600, 450), plot_title=\"Data Distribution for O₃\")\n\n\n\n\n\n# load in spectrometer wavelengths\nhr4000_df = CSV.File(\"../hr4000_wavelengths.txt\") |> DataFrame ; \n\n\nprintln(nrow(hr4000_df))\nprintln(maximum(hr4000_df.λ))\nprintln(minimum(hr4000_df.λ))\n\n3648\n1120.216\n194.249\n\n\nIt appears that GPR can not handle identical records with different target values, i.e. if \\(x\\_1 = [1.0, 2.0]\\) and \\(x\\_2 = [1.0, 2.0]\\), while \\(y\\_1 = 7.2\\) and \\(y_2 = 7.89\\). To deal with this, let’s first group the dataframe by temperature, and average across identical wavelength values.\n\ngdfs = groupby(df, :T)\n\nres_dfs = []\n\nfor gdf ∈ gdfs\n    gdf_by_λ = groupby(gdf, :λ)\n    push!(res_dfs, combine(gdf_by_λ, [:σ, :T] .=> mean, renamecols = false))\nend\n\ndf_unique = vcat(res_dfs...)\n\n\n191437×3 DataFrame191412 rows omittedRowλσTFloat64Float64Float641145.955.23e-18298.02148.14.47e-18298.03150.353.69e-18298.04152.652.93e-18298.05155.02.19e-18298.06157.451.63e-18298.07160.01.2e-18298.08162.69.77e-19298.09165.38.66e-19298.010168.18.14e-19298.011170.958.17e-19298.012173.158.57e-19298.013174.658.4e-19298.0⋮⋮⋮⋮191426721.755.17e-22290.0191427724.784.79e-22290.0191428727.944.51e-22290.0191429731.114.22e-22290.0191430734.133.87e-22290.0191431737.443.74e-22290.0191432740.63.77e-22290.0191433743.93.76e-22290.0191434747.213.66e-22290.0191435749.83.59e-22290.0191436325.1261.647e-20294.09191437253.71.1365e-17297.3\n\n\n\nfunction representative_rand_sample(column::AbstractVector, nbins::Int, npoints::Int)\n    n_per_bin = floor(Int, npoints/nbins)\n    hist = fit(Histogram, column, nbins=nbins)\n    bin_edges = hist.edges[1]\n    \n    idx_out = []\n    \n    # loop over each bin\n    for i ∈ 1:size(bin_edges, 1)-1\n        bin_idxs = findall(ξ -> bin_edges[i] < ξ && ξ < bin_edges[i+1], column)\n        n_to_sample = minimum([n_per_bin, size(bin_idxs, 1)])\n        idx_res = sample(bin_idxs, n_to_sample, replace=false)\n        push!(idx_out, idx_res)  # sample without replacement\n    end\n\n    return unique(vcat(idx_out...))\nend\n\nλ_idxs = representative_rand_sample(df_unique.λ, 1000, 1500)\nσ_idxs = representative_rand_sample(log10.(df_unique.σ), 500, 2000) \n\n# λ_idxs = representative_rand_sample(df_unique.λ, 1250, 1500)\n# σ_idxs = representative_rand_sample(log10.(df_unique.σ), 750, 2000) \n\n\nidxs_res = shuffle(unique(vcat(λ_idxs, σ_idxs)))\n\n\ndf_sampled = df_unique[idxs_res, :]\n\n\n2387×3 DataFrame2362 rows omittedRowλσTFloat64Float64Float641386.594.21367e-24293.02407.642.12243e-23295.03415.192.91879e-23295.04213.787.43547e-19293.05689.7131.17499e-21293.06364.754.80842e-23295.071028.981.80185e-24293.08894.595.17471e-23293.09260.61.24e-17290.010590.984.48892e-21295.011287.631.86089e-18295.012916.921.64895e-23293.013409.062.56583e-23295.0⋮⋮⋮⋮2376274.176.07182e-18293.023771039.852.66417e-24293.023781088.51.6595e-24293.02379322.472.14004e-20295.02380596.494.80395e-21293.02381633.563.31589e-21293.02382254.291.28e-17290.02383701.038.4583e-22295.02384372.6993.77e-23293.02385489.538.03712e-22295.02386741.6055.05682e-22293.023871081.371.82854e-24293.0\n\n\n\np1 = histogram(df_sampled.λ, xlabel=\"λ\", ylabel=\"counts\", label=\"\", margin=10Plots.mm)\np2 = histogram(df_sampled.T, xlabel=\"T\", label=\"\", margin=10Plots.mm)\np3 = histogram(log10.(df_sampled.σ), xlabel=\"log10(σ)\", label=\"\", margin=10Plots.mm)\n#p3 = histogram(df.σ, xlabel=\"σ\", ylabel=\"counts\")\n\nphist = plot(p1, p2, p3, layout=(1,3), size=(1600, 450), plot_title=\"Subsampled Data Distribution for O₃\")\n\n\n\n\n\np = scatter(\n    df_sampled.λ,\n    log10.(df_sampled.σ),\n    zcolor=df_sampled.T,\n    ms=3, \n    msw=0,\n    xlabel=\"λ [$(λ_units)]\",\n    ylabel=\"log10(σ) [cm²]\",\n    alpha=0.7,\n    rightmargin=10Plots.mm,\n    colorbar_title=\"T [$(T_units)]\",\n    title=\"Absorption Cross Section for O₃\",\n    label=\"\",\n)\n\n\n\n\n\nusing KernelFunctions\nusing ParameterHandling\n\n#kernel(θ) = θ.σ₁²*(SqExponentialKernel() ∘ ScaleTransform(1/(2(θ.ℓ₁)^2))) + θ.σ₂²*(Matern32Kernel() ∘ ScaleTransform(1/(2(θ.ℓ₂)^2)))\n#θ_init = (;σ₁²=positive(0.1), ℓ₁=positive(0.1), σ₂²=positive(0.1), ℓ₂=positive(0.1),)\n# gpr = GPR(\n    # k = kernel,\n    # θ_init = θ_init, \n    # σ²=1e-5\n# )\n# mach = machine(gpr, Xtrain, ytrain) |> fit!\nprintln(nrow(df_sampled))\nXtrain = df_sampled[1:2000, [:λ, :T]]\nXtest = df_sampled[2001:end, [:λ, :T]]\n\nytrain = log10.(df_sampled.σ[1:2000])\nytest = log10.(df_sampled.σ[2001:end])\n\n#gpr = GPR(σ²=0.00001)\ngpr = GPR(σ²=20.0)\n#gpr = GPR(σ²=10.0)\n\nmach = machine(gpr, Xtrain, ytrain) |> fit!\n\n2387\n\n\n[ Info: Training machine(GPR(μ = 0.0, …), …).\n\n\nIter     Function value   Gradient norm \n     0     1.893567e+04     1.091244e+04\n * time: 0.01671600341796875\n     1     1.405405e+04     9.992352e+02\n * time: 6.924190044403076\n     2     4.565267e+03     5.969289e+02\n * time: 11.398995876312256\n     3     4.454063e+03     7.739619e+02\n * time: 13.067230939865112\n     4     4.255704e+03     6.834836e+01\n * time: 14.137372016906738\n     5     4.253254e+03     1.680068e+00\n * time: 15.80565094947815\n     6     4.253250e+03     1.309147e+00\n * time: 17.47168207168579\n     7     4.252584e+03     9.438136e-01\n * time: 20.816890954971313\n     8     4.252583e+03     4.750737e-02\n * time: 22.50261402130127\n     9     4.252583e+03     5.841785e-04\n * time: 24.17035984992981\n    10     4.252583e+03     1.603219e-06\n * time: 25.295099020004272\n    11     4.252583e+03     3.395776e-08\n * time: 26.96492886543274\n    12     4.252583e+03     3.395776e-08\n * time: 28.109287977218628\n    13     4.252583e+03     7.167757e-07\n * time: 30.312039852142334\n    14     4.252583e+03     1.868200e-07\n * time: 34.78383708000183\n    15     4.252583e+03     1.064816e-05\n * time: 37.676944971084595\n    16     4.252583e+03     2.192087e-04\n * time: 44.19379806518555\n    17     3.313514e+03     9.830546e+02\n * time: 53.10497808456421\n    18     3.310122e+03     9.828801e+02\n * time: 58.697805881500244\n    19     2.801403e+03     9.769818e+02\n * time: 60.93287801742554\n    20    -2.844720e+02     1.384003e+03\n * time: 66.63852190971375\n\n\n┌ Error: Problem fitting the machine machine(GPR(μ = 0.0, …), …). \n└ @ MLJBase ~/.julia/packages/MLJBase/uxwHr/src/machines.jl:682\n[ Info: Running type checks... \n[ Info: Type checks okay. \n\n\nLoadError: PosDefException: matrix is not positive definite; Cholesky factorization failed.\n\n\n\ny_pred_train = predict_mean(mach, Xtrain);\ny_pred_test = predict_mean(mach, Xtest);\n\nLoadError: UndefVarError: mach not defined\n\n\nnow let’s collect some more points in order to evaluate our model using indices other than the ones used during training.\n\np = scatterresult(\n    ytrain, y_pred_train,\n    ytest, y_pred_test,\n    xlabel=\"log10(σ) Truth\",\n    ylabel=\"log10(σ) Fit\",\n    plot_title=\"O₃ GPR Fit\"\n)\n\nsavefig(\"O3_scatter.png\")\nsavefig(\"O3_scatter.pdf\")\n\ndisplay(p)\n\nLoadError: UndefVarError: y_pred_train not defined\n\n\n\np = quantilequantile(\n    ytrain, y_pred_train,\n    ytest, y_pred_test,\n    xlabel=\"log10(σ) Truth\",\n    ylabel=\"log10(σ) Fit\",\n    title=\"O₃ GPR Fit\"\n)\n\nsavefig(\"O3_quantile.png\")\nsavefig(\"O3_quantile.pdf\")\n\ndisplay(p)\n\nLoadError: UndefVarError: y_pred_train not defined\n\n\n\np1 = scatter(\n    Xtrain.λ,\n    y_pred_train,\n    zcolor=Xtrain.T,\n    ms=3, \n    msw=0,\n    xlabel=\"λ [$(λ_units)]\",\n    ylabel=\"log10(σ) [cm²]\",\n    alpha=0.7,\n    rightmargin=10Plots.mm,\n    colorbar_title=\"T [$(T_units)]\",\n    label=\"Training Data\",\n)\n\nscatter!(\n    Xtest.λ,\n    y_pred_test,\n    zcolor=Xtest.T,\n    ms=3, \n    msw=0,\n    # markershape=:rect,\n    markershape=:star5,\n    xlabel=\"λ [$(λ_units)]\",\n    ylabel=\"log10(σ) [cm²]\",\n    alpha=0.7,\n    rightmargin=10Plots.mm,\n    colorbar_title=\"T [$(T_units)]\",\n    title=\"Predicted O₃ Cross Section\",\n    label=\"Testing Data\",\n)\n\nsavefig(\"predicted_O3_data.png\")\nsavefig(\"predicted_O3_data.pdf\")\n\ndisplay(p1)\n\nLoadError: UndefVarError: y_pred_train not defined\n\n\nFinally, let’s try prediction at the desired wavelength bins:\n\nfunction predict_logσ(T, λs, mach)\n    Xout = copy(λs)\n    Tout = T\n    Xout[!, :T] = [Tout for _ ∈ 1:nrow(λs)]\n    return predict_mean(mach, Xout)\nend\n\n\np = plot()\n\nTs = 292.0:0.5:300.0\n# cs = cgrad(:thermal, Ts, categorical = true)\ncs = cgrad(:roma, size(Ts,1), categorical = true, rev=true)\ni = 1\nfor T ∈ Ts\n    logσ = predict_logσ(T, hr4000_df, mach)\n    plot!(\n        hr4000_df.λ, logσ,\n        linewidth=2,\n        alpha=0.5,\n        color=cs[i],\n        label=\"$(T) K\",\n        legend=:outertopright,\n    )\n    i+=1\nend\n\nxlabel!(\"λ [nm]\")\nylabel!(\"log10(σ) [cm²]\")\ntitle!(\"Predicted Cross Section at Spectrometer λ's\")\nsavefig(\"O3_σ_vs_T.png\")\nsavefig(\"O3_σ_vs_T.pdf\")\n\ndisplay(p)\n\nLoadError: UndefVarError: mach not defined"
  },
  {
    "objectID": "cross_sections/H2O2.html",
    "href": "cross_sections/H2O2.html",
    "title": "Cross Sections, Quantum Yields, and Photolysis Rates",
    "section": "",
    "text": "\\(\\mathrm{H_2O_2}\\)"
  },
  {
    "objectID": "cross_sections/O3_old.html",
    "href": "cross_sections/O3_old.html",
    "title": "Cross Sections, Quantum Yields, and Photolysis Rates",
    "section": "",
    "text": "\\(\\mathrm{O_3}\\)\n\nusing Plots, MLPlotRecipes \nusing HDF5 \nusing Tables\nusing MLJ, MLJGaussianProcesses\n\n\nh5_path = \"../../data/photolysis_data.h5\" \nisfile(h5_path)\n\ntrue\n\n\nread in the HDF5 file with our data and identify the cross section data we need:\n\nh5 = h5open(h5_path, \"r\");\n\n\ncross_sections = h5[\"cross-sections\"]\n\n\"O3\" ∈ keys(cross_sections)\n\nO3_data = cross_sections[\"O3\"]\n\n📂 HDF5.Group: /cross-sections/O3 (file: ../../data/photolysis_data.h5)\n├─ 🏷️ T1_units\n├─ 🏷️ T2_units\n├─ 🏷️ category\n├─ 🏷️ formula\n├─ 🏷️ Δσ_units\n├─ 🏷️ λ_units\n├─ 🏷️ σ_units\n├─ 🔢 T1\n├─ 🔢 T2\n├─ 🔢 source_idx\n├─ 📂 source_info\n│  ├─ 📂 1\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 10\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 100\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 101\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 102\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 103\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 104\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 105\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 106\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 107\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 108\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 109\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 11\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 110\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 111\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 112\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 113\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 114\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 115\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 116\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 117\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 118\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 119\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 12\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 120\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  └─ (224 more children)\n├─ 🔢 Δσ\n├─ 🔢 λ\n└─ 🔢 σ\n\n\n\nλs = read(O3_data, \"λ\")\nσs = read(O3_data, \"σ\")\nT1s = read(O3_data, \"T1\")\nT2s = read(O3_data, \"T2\")\nsource_idx = read(O3_data, \"source_idx\")\n\n\n\nT_units = read_attribute(O3_data, \"T1_units\")\ncategory= read_attribute(O3_data, \"category\")\nformula= read_attribute(O3_data, \"formula\")\nλ_units = read_attribute(O3_data, \"λ_units\")\nσ_units = read_attribute(O3_data, \"σ_units\")\n\nprintln(T_units)\nprintln(category)\nprintln(formula)\nprintln(λ_units)\nprintln(σ_units)\n\nclose(h5)\n\nK\nOzone\nO3\nnm\ncm^2\n\n\n\nprintln(size(λs))\nprintln(size(T1s))\nprintln(size(σs))\n\n(2591145,)\n(249,)\n(2591145,)\n\n\n\nusing DataFrames\n\n\nfunction generate_data_table(λs, σs, T1s, T2s, source_idx; σ_lb=1e-25, σ_ub=1.0, T_lb=290.0, T_ub=305.0, λ_lb=200.0, λ_ub=1100.0)\n    Tout = [T1s[source_idx[i]] for i ∈ 1:size(source_idx, 1)]\n    \n    # we want T2 to be NaN\n    idx = [ i for i  ∈ 1:size(source_idx,1) if isnan(T2s[source_idx[i]])]\n    λout = λs[idx]\n    σout = σs[idx]\n    Tout = Tout[idx]\n    source_out = source_idx[idx]\n\n    # we want T between T_lb and T_ub\n    idx2 = [i for i ∈ 1:size(λout,1) if (T_lb ≤ Tout[i] && Tout[i] ≤ T_ub)]\n    λout = λout[idx2]\n    σout = σout[idx2]\n    Tout = Tout[idx2]\n    source_out = source_out[idx2]\n\n    # we want σ to not be NaN and greater than 0.\n    idx3 = [i for i ∈ 1:size(σout,1) if !isnan(σout[i]) && (σout[i] > σ_lb) && (σout[i] ≤ σ_ub)]\n    λout = λout[idx3]\n    σout = σout[idx3]\n    Tout = Tout[idx3]\n    source_out = source_out[idx3]\n\n    # we want λ to be between λ_lb and λ_ub\n    idx4 = [i for i ∈ 1:size(λout,1) if (λ_lb ≤ λout[i] && λout[i] ≤ λ_ub)]\n    λout = λout[idx4]\n    σout = σout[idx4]\n    Tout = Tout[idx4]\n    source_out = source_out[idx4]\n   \n    # idxs = [i for i ∈ 1:size(source_idx, 1) if (T_lb < Temps[i] && Temps[i] < T_ub) && (!isnan(σs[i])) && (σs[i] ≥ 0.0) && (λ_lb ≤ λs[i] && λs[i] ≤ λ_ub) && isnan(T2s[source_idx[i]])]\n\n    # 5. Create table with data containing the good values\n    #data_table = Tables.columntable((; λ=λout, σ=σout, T=Tout, source_id=source_out))\n    df =  DataFrame(λ=λout, σ=σout, T=Tout, source_id=source_out)\n    return df\nend\n\n\ndf = generate_data_table(λs, σs, T1s, T2s, source_idx)\nprintln(nrow(df))\ndescribe(df)\n\n365012\n\n\n\n4×7 DataFrameRowvariablemeanminmedianmaxnmissingeltypeSymbolFloat64RealFloat64RealInt64DataType1λ566.604200.0541.3331100.00Float642σ9.09226e-193.4377e-258.00578e-221.41e-170Float643T293.785290.0293.0300.00Float644source_id130.4961175.02490Int64\n\n\n\np = scatter(\n    df.λ[1:5:end],\n    df.T[1:5:end],\n    zcolor=log10.(df.σ[1:5:end]),\n    ms=3,\n    msw=0, \n    xlabel=\"λ [$(λ_units)]\",\n    ylabel=\"T [$(T_units)]\",\n    label=\"data\", \n    alpha=0.7, \n    rightmargin=10Plots.mm,\n    colorbar_title=\"log10(σ) [cm²]\",\n    title=\"Absorption Cross Section Data for O₃\",\n)\n\nsavefig(\"O3_log10σ.png\")\n\ndisplay(p)\n\n\n\n\n\np = scatter(\n    df.λ[1:5:end],\n    df.T[1:5:end],\n    zcolor=df.σ[1:5:end],\n    ms=3,\n    msw=0, \n    xlabel=\"λ [$(λ_units)]\",\n    ylabel=\"T [$(T_units)]\",\n    label=\"data\", \n    alpha=0.7, \n    rightmargin=10Plots.mm,\n    colorbar_title=\"σ [cm²]\",\n    title=\"Absorption Cross Section Data for O₃\",\n)\n\nsavefig(\"O3_σ.png\")\ndisplay(p)\n\n\n\n\n\np1 = histogram(df.λ, xlabel=\"λ\", ylabel=\"counts\", label=\"\", margin=10Plots.mm)\np2 = histogram(df.T, xlabel=\"T\", label=\"\", margin=10Plots.mm)\np3 = histogram(log10.(df.σ), xlabel=\"log10(σ)\", label=\"\", margin=10Plots.mm)\n#p3 = histogram(df.σ, xlabel=\"σ\", ylabel=\"counts\")\n\nphist = plot(p1, p2, p3, layout=(1,3), size=(1600, 450))\nsavefig(\"histograms.png\")\ndisplay(phist)\n\n\n\n\nWe can now thik of this as providing our training data. The goal is to learn a function \\(f:(\\lambda, T) \\mapsto \\sigma\\). Our model of choice will be to use a gaussian process.\nFirst let’s split up the data into training and testing pairs.\n\n\n\n\n\n\nImportant\n\n\n\nWe should think carefully about how to do this. The most important information is probably in regions where there is rapid changes. We can also examine fitting \\(\\log(\\sigma)\\) if \\(\\sigma\\) alone is to sharply peaked…\n\n\n\nn = nrow(df)\ntrain_frac = 0.01\n\nidxs = shuffle(1:n)\nntrain = round(Int, (1.0 - train_frac)*n)\n\nidx_train = idxs[1:ntrain]\nidx_test = idxs[ntrain+1:end]\n\n\ndf_train = df[idx_train, :]\ndf_test = df[idx_test, :]\n\n\n3650×4 DataFrame3625 rows omittedRowλσTsource_idFloat64Float64Float64Int641668.81.73771e-21293.01752852.591.44575e-22293.01763724.05.523e-22295.0534627.9323.746e-21293.02215234.746.18845e-18293.01766599.765.13058e-21295.0487885.134.37375e-23293.01758408.4087.938e-23293.02219800.281.55912e-22295.04810219.241.59597e-18293.017511359.257.69109e-23293.017612803.51.52178e-22295.04813442.361.77009e-22293.0176⋮⋮⋮⋮⋮3639364.74.70862e-23295.0483640591.24.4885e-21295.0523641887.024.80926e-23293.01753642711.437.09813e-22293.01763643750.744.15931e-22295.0483644828.41.10054e-22295.0523645657.452.17106e-21293.01753646964.758.73398e-24293.01763647220.091.73649e-18293.01753648957.291.33572e-23293.01763649257.01.09734e-17295.05036501052.213.73944e-24293.0175\n\n\n\nXtrain = df_train[!, [:λ, :T]]\nytrain = df_train[!, :σ]\n\nXtest = df_test[!, [:λ, :T]]\nytest = df_test[!, :σ]\n\n3650-element Vector{Float64}:\n 1.73771e-21\n 1.445749e-22\n 5.523e-22\n 3.746e-21\n 6.188448e-18\n 5.13058e-21\n 4.37375e-23\n 7.938e-23\n 1.55912e-22\n 1.59597e-18\n 7.691088e-23\n 1.52178e-22\n 1.770092e-22\n ⋮\n 4.70862e-23\n 4.4885e-21\n 4.80926e-23\n 7.098132e-22\n 4.15931e-22\n 1.10054e-22\n 2.17106e-21\n 8.733981e-24\n 1.73649e-18\n 1.335722e-23\n 1.09734e-17\n 3.73944e-24\n\n\n\ngpr = GPR()\n#mach = machine(gpr, Xtrain, ytrain) |> fit!\n\n\nGPR(\n  μ = 0.0, \n  k = MLJGaussianProcesses.default_kernel, \n  θ_init = (σf² = ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8), ℓ = ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8)), \n  σ² = 1.0e-6, \n  optimizer = Optim.LBFGS{Nothing, LineSearches.InitialStatic{Float64}, LineSearches.HagerZhang{Float64, Base.RefValue{Bool}}, Optim.var\"#19#21\"}(10, LineSearches.InitialStatic{Float64}\n  alpha: Float64 1.0\n  scaled: Bool false\n, LineSearches.HagerZhang{Float64, Base.RefValue{Bool}}\n  delta: Float64 0.1\n  sigma: Float64 0.9\n  alphamax: Float64 Inf\n  rho: Float64 5.0\n  epsilon: Float64 1.0e-6\n  gamma: Float64 0.66\n  linesearchmax: Int64 50\n  psi3: Float64 0.1\n  display: Int64 0\n  mayterminate: Base.RefValue{Bool}\n, nothing, Optim.var\"#19#21\"(), Optim.Flat(), true))"
  },
  {
    "objectID": "cross_sections/H2O2_old.html",
    "href": "cross_sections/H2O2_old.html",
    "title": "Cross Sections, Quantum Yields, and Photolysis Rates",
    "section": "",
    "text": "\\(\\mathrm{H_2O_2}\\)\n\nusing Plots, MLPlotRecipes \nusing HDF5 \nusing Tables\nusing MLJ, MLJGaussianProcesses\n\n\nh5_path = \"../../data/photolysis_data.h5\" \nisfile(h5_path)\n\ntrue\n\n\nread in the HDF5 file with our data and identify the cross section data we need:\n\nh5 = h5open(h5_path, \"r\");\n\n\ncross_sections = h5[\"cross-sections\"]\n\n\"H2O2\" ∈ keys(cross_sections)\n\nH2O2_data = cross_sections[\"H2O2\"]\n\n📂 HDF5.Group: /cross-sections/H2O2 (file: ../../data/photolysis_data.h5)\n├─ 🏷️ T1_units\n├─ 🏷️ T2_units\n├─ 🏷️ category\n├─ 🏷️ formula\n├─ 🏷️ sub-category\n├─ 🏷️ Δσ_units\n├─ 🏷️ λ_units\n├─ 🏷️ σ_units\n├─ 🔢 T1\n├─ 🔢 T2\n├─ 🔢 source_idx\n├─ 📂 source_info\n│  ├─ 📂 1\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 10\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 11\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 12\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 13\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 14\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 15\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 16\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 17\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 18\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 19\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 2\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 20\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 21\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 22\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 23\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 24\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 25\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 26\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 27\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 28\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 29\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 3\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 30\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  ├─ 📂 4\n│  │  ├─ 🏷️ author(year)\n│  │  ├─ 🏷️ comments\n│  │  ├─ 🏷️ doi\n│  │  └─ 🏷️ download_url\n│  └─ (5 more children)\n├─ 🔢 Δσ\n├─ 🔢 λ\n└─ 🔢 σ\n\n\n\nλs = read(H2O2_data, \"λ\")\nσs = read(H2O2_data, \"σ\")\nT1s = read(H2O2_data, \"T1\")\nT2s = read(H2O2_data, \"T2\")\nsource_idx = read(H2O2_data, \"source_idx\")\n\n\n\nT_units = read_attribute(H2O2_data, \"T1_units\")\ncategory= read_attribute(H2O2_data, \"category\")\nformula= read_attribute(H2O2_data, \"formula\")\nλ_units = read_attribute(H2O2_data, \"λ_units\")\nσ_units = read_attribute(H2O2_data, \"σ_units\")\n\nprintln(T_units)\nprintln(category)\nprintln(formula)\nprintln(λ_units)\nprintln(σ_units)\n\nclose(h5)\n\nK\nPeroxides\nH2O2\nnm\ncm^2\n\n\n\nprintln(size(λs))\nprintln(size(T1s))\nprintln(size(σs))\n\n(1164,)\n(30,)\n(1164,)\n\n\n\nusing DataFrames\n\n\nfunction generate_data_table(λs, σs, T1s, T2s, source_idx; σ_lb=1e-25, σ_ub=1.0, T_lb=290.0, T_ub=305.0, λ_lb=200.0, λ_ub=1100.0)\n    Tout = [T1s[source_idx[i]] for i ∈ 1:size(source_idx, 1)]\n    \n    # we want T2 to be NaN\n    idx = [ i for i  ∈ 1:size(source_idx,1) if isnan(T2s[source_idx[i]])]\n    λout = λs[idx]\n    σout = σs[idx]\n    Tout = Tout[idx]\n    source_out = source_idx[idx]\n\n    # we want T between T_lb and T_ub\n    idx2 = [i for i ∈ 1:size(λout,1) if (T_lb ≤ Tout[i] && Tout[i] ≤ T_ub)]\n    λout = λout[idx2]\n    σout = σout[idx2]\n    Tout = Tout[idx2]\n    source_out = source_out[idx2]\n\n    # we want σ to not be NaN and greater than 0.\n    idx3 = [i for i ∈ 1:size(σout,1) if !isnan(σout[i]) && (σout[i] > σ_lb) && (σout[i] ≤ σ_ub)]\n    λout = λout[idx3]\n    σout = σout[idx3]\n    Tout = Tout[idx3]\n    source_out = source_out[idx3]\n\n    # we want λ to be between λ_lb and λ_ub\n    idx4 = [i for i ∈ 1:size(λout,1) if (λ_lb ≤ λout[i] && λout[i] ≤ λ_ub)]\n    λout = λout[idx4]\n    σout = σout[idx4]\n    Tout = Tout[idx4]\n    source_out = source_out[idx4]\n   \n    # idxs = [i for i ∈ 1:size(source_idx, 1) if (T_lb < Temps[i] && Temps[i] < T_ub) && (!isnan(σs[i])) && (σs[i] ≥ 0.0) && (λ_lb ≤ λs[i] && λs[i] ≤ λ_ub) && isnan(T2s[source_idx[i]])]\n\n    # 5. Create table with data containing the good values\n    #data_table = Tables.columntable((; λ=λout, σ=σout, T=Tout, source_id=source_out))\n    df =  DataFrame(λ=λout, σ=σout, T=Tout, source_id=source_out)\n    return df\nend\n\n\ndf = generate_data_table(λs, σs, T1s, T2s, source_idx)\nprintln(nrow(df))\ndescribe(df)\n\n370\n\n\n\n4×7 DataFrameRowvariablemeanminmedianmaxnmissingeltypeSymbolFloat64RealFloat64RealInt64DataType1λ267.227200.0260.0400.00Float642σ1.24797e-192.5e-245.565e-205.47e-190Float643T297.984295.0298.0300.00Float644source_id12.3324111.0290Int64\n\n\n\np = scatter(\n    df.λ,\n    df.T,\n    zcolor=log10.(df.σ),\n    ms=3,\n    msw=0, \n    xlabel=\"λ [$(λ_units)]\",\n    ylabel=\"T [$(T_units)]\",\n    label=\"data\", \n    alpha=0.7, \n    rightmargin=10Plots.mm,\n    colorbar_title=\"log10(σ) [cm²]\",\n    title=\"Absorption Cross Section Data for H₂O₂\",\n)\n\n\n\n\n\np = scatter(\n    df.λ,\n    df.T,\n    zcolor=df.σ,\n    ms=3,\n    msw=0, \n    xlabel=\"λ [$(λ_units)]\",\n    ylabel=\"T [$(T_units)]\",\n    label=\"data\", \n    alpha=0.7, \n    rightmargin=10Plots.mm,\n    colorbar_title=\"σ [cm²]\",\n    title=\"Absorption Cross Section Data for H₂O₂\",\n)\n\n\n\n\n\np1 = histogram(df.λ, xlabel=\"λ\", ylabel=\"counts\", label=\"\", margin=10Plots.mm)\np2 = histogram(df.T, xlabel=\"T\", label=\"\", margin=10Plots.mm)\np3 = histogram(log10.(df.σ), xlabel=\"log10(σ)\", label=\"\", margin=10Plots.mm)\n#p3 = histogram(df.σ, xlabel=\"σ\", ylabel=\"counts\")\n\nphist = plot(p1, p2, p3, layout=(1,3), size=(1600, 450))\n\n\n\n\nWe can now thik of this as providing our training data. The goal is to learn a function \\(f:(\\lambda, T) \\mapsto \\sigma\\). Our model of choice will be to use a gaussian process.\nFirst let’s split up the data into training and testing pairs.\n\n\n\n\n\n\nImportant\n\n\n\nWe should think carefully about how to do this. The most important information is probably in regions where there is rapid changes. We can also examine fitting \\(\\log(\\sigma)\\) if \\(\\sigma\\) alone is to sharply peaked…\n\n\n\nn = nrow(df)\ntrain_frac = 0.1\n\nidxs = shuffle(1:n)\nntrain = round(Int, (1.0 - train_frac)*n)\n\nidx_train = idxs[1:ntrain]\nidx_test = idxs[ntrain+1:end]\n\n\ndf_train = df[idx_train, :]\ndf_test = df[idx_test, :]\n\n\n37×4 DataFrame12 rows omittedRowλσTsource_idFloat64Float64Float64Int641220.02.58e-19298.062335.01.0e-21298.083340.08.2e-22297.0284255.02.46e-20300.075235.01.56e-19297.0286220.02.45e-19298.0137345.05.0e-22298.088330.01.3e-21296.0119340.08.0e-22300.0710204.04.69e-19298.0411260.05.7e-20296.01212325.02.08e-21297.02813345.05.1e-22300.015⋮⋮⋮⋮⋮26245.561.53e-19298.0227207.04.3e-19298.0428340.07.0e-22298.0829290.07.66e-21298.02230315.03.29e-21297.02831270.03.31e-20300.01532345.04.0e-22298.01333277.944.68e-20298.0234340.01.0e-21296.01235370.01.75e-23298.02236345.06.0e-22296.01137218.03.04e-19298.04\n\n\n\nXtrain = df_train[!, [:λ, :T]];\nytrain = log10.(df_train[!, :σ]);\n\nXtest = df_test[!, [:λ, :T]];\nytest = log10.(df_test[!, :σ]);\n\n\nusing Pkg\nPkg.add(\"ParameterHandling\")\n\n   Resolving package versions...\n    Updating `~/gitrepos/ActivePure/Photolysis.jl/mcm/Project.toml`\n  [2412ca09] + ParameterHandling v0.4.6\n  No Changes to `~/gitrepos/ActivePure/Photolysis.jl/mcm/Manifest.toml`\n\n\n\nusing KernelFunctions\nusing ParameterHandling\n\nkernel(θ) = θ.σ₁²*(SqExponentialKernel() ∘ ScaleTransform(1/(2(θ.ℓ₁)^2))) + θ.σ₂²*(Matern32Kernel() ∘ ScaleTransform(1/(2(θ.ℓ₂)^2)))\nθ_init = (;σ₁²=positive(0.1), ℓ₁=positive(0.1), σ₂²=positive(0.1), ℓ₂=positive(0.1),)\ngpr = GPR(\n    k = kernel,\n    θ_init = θ_init, \n    σ²=1e-7\n)\n# mach = machine(gpr, Xtrain, ytrain) |> fit!\nmach = machine(gpr, df[!, [:λ, :T]], log10.(df.σ)) |> fit!\n\n[ Info: Training machine(GPR(μ = 0.0, …), …).\n\n\nIter     Function value   Gradient norm \n     0     5.520506e+07     4.683357e+07\n * time: 0.00015687942504882812\n     1     8.397008e+04     1.850000e+02\n * time: 0.1980438232421875\n     2     2.910224e+03     8.683632e+06\n * time: 0.7028379440307617\n     3     2.339622e+03     2.052704e+13\n * time: 2.0466439723968506\n     4     2.284220e+03     1.830463e+02\n * time: 2.2037007808685303\n     5     1.831759e+03     1.594123e+02\n * time: 2.2866809368133545\n     6     1.672186e+03     1.028704e+02\n * time: 2.4727489948272705\n     7     1.640279e+03     6.596509e+01\n * time: 2.5251169204711914\n     8     1.624784e+03     8.922078e+00\n * time: 2.5702309608459473\n     9     1.624559e+03     3.020414e-01\n * time: 2.632138967514038\n    10     1.624521e+03     4.003218e+00\n * time: 2.754595994949341\n    11     1.559835e+03     1.059581e+02\n * time: 2.8942248821258545\n    12     1.558517e+03     1.021434e+02\n * time: 3.015133857727051\n    13     1.312760e+03     9.561306e+01\n * time: 3.1111948490142822\n    14     9.228737e+02     6.320437e+01\n * time: 3.2740349769592285\n    15     8.885289e+02     3.601162e+01\n * time: 3.3891518115997314\n    16     8.389134e+02     1.604713e+01\n * time: 3.4401509761810303\n    17     8.335698e+02     3.855661e+00\n * time: 3.5000698566436768\n    18     8.334245e+02     3.626831e-02\n * time: 3.545395851135254\n    19     8.334245e+02     1.353522e-02\n * time: 3.5983259677886963\n    20     4.889816e+02     9.798242e+01\n * time: 3.8875157833099365\n    21     4.871506e+02     9.687293e+01\n * time: 3.931220769882202\n    22     3.474097e+02     5.279476e+01\n * time: 4.0060179233551025\n    23     3.246644e+02     2.937024e+01\n * time: 4.04784083366394\n    24     3.151731e+02     5.276264e+00\n * time: 4.09749698638916\n    25     3.150298e+02     1.377514e+00\n * time: 4.1406848430633545\n    26     3.150150e+02     2.884675e-01\n * time: 4.1958818435668945\n    27     3.149679e+02     1.472008e+00\n * time: 4.3132407665252686\n    28     3.149543e+02     4.499126e-02\n * time: 4.394497871398926\n    29     3.149542e+02     3.037940e-03\n * time: 4.472301959991455\n    30     3.149542e+02     2.376821e-05\n * time: 4.53503680229187\n    31     3.149542e+02     8.836690e-08\n * time: 4.6040449142456055\n    32     3.149542e+02     3.455128e-08\n * time: 4.6613547801971436\n    33     3.149542e+02     3.873630e-09\n * time: 4.705882787704468\n\n\ntrained Machine; caches model-specific representations of data\n  model: GPR(μ = 0.0, …)\n  args: \n    1:  Source @412 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @762 ⏎ AbstractVector{Continuous}\n\n\n\nrpt = report(mach)\n\n(summary = \"L-BFGS\",\n minimizer = [0.08517875349265944, -1.2829398398521916e11, 5.942946238151963, 40.71001341650538, -2.8475008475089973],\n minimum = 314.9541966975367,\n iterations = 33,\n converged = true,)\n\n\n\ny_pred_train = predict_mean(mach, Xtrain)\ny_pred_test = predict_mean(mach, Xtest)\n\n\npy_train = predict(mach, Xtrain)\npy_test = predict(mach, Xtest)\n\npy_train[1]\n\nDistributions.Normal{Float64}(μ=-21.399818476559005, σ=0.10712744012232024)\n\n\n\nscatterresult(\n    ytrain, y_pred_train,\n    ytest, y_pred_test,\n    xlabel=\"log10(σ) Truth\",\n    ylabel=\"log10(σ) Fit\",\n    plot_title=\"H₂O₂ GPR Fit\"\n)\n\n\n\n\n\nquantilequantile(\n    ytrain, y_pred_train,\n    ytest, y_pred_test,\n    xlabel=\"log10(σ) Truth\",\n    ylabel=\"log10(σ) Fit\",\n    title=\"H₂O₂ Fit Quantile-Quantile\"\n)\n\n\n\n\nNow let’s plot the resulting function on a grid:\n\nλ₁ = minimum(df.λ)\nλ₂ = maximum(df.λ)\nT₁ = minimum(df.T)\nT₂ = maximum(df.T)\n\nΔλ = λ₂ - λ₁\nΔT = T₂ - T₁\n\nfudge_fac = 0.1\nλ₁ = λ₁ - fudge_fac*Δλ\nλ₂ = λ₂ + fudge_fac*Δλ\nT₁ = T₁ - fudge_fac*ΔT\nT₂ = T₂ + fudge_fac*ΔT\n\n\nλs = range(λ₁, λ₂, length=200)\nTs = range(T₁, T₂, length=50)\n\n# resulting shape should be reshaped to 500 by 50\nλ_grid = [λ for λ∈λs, T∈Ts]\nT_grid = [T for λ∈λs, T∈Ts]\ndf_grid = DataFrame(λ=vcat(λ_grid...), T=vcat(T_grid...))\nσ_grid = reshape(predict_mean(mach, df_grid),(200,50))\n\nheatmap(λs, Ts, σ_grid)\n\nscatter!(\n    df.λ,\n    df.T,\n    zcolor=log10.(df.σ),\n    ms=4,\n    label=\"data\", \n    rightmargin=10Plots.mm,\n    colorbar_title=\"σ [cm²]\",\n)\n\n298.0\n\n\n\nλs = range(λ₁, λ₂, length=500)\nTs = [298.0 for _ ∈ 1:500]\ndf_pred = DataFrame(λ=λs, T=Ts)\nσ_preds = predict_mean(mach, df_pred)\n\nplot(λs, σ_preds)\n\n\n\n\n\nfunction make_training_data(n, ν)\n    xs = reshape(collect(range(0.0, stop=1.0, length=500)), (1,500))\n\n    y(x) = (exp(-x/(0.5)^2) * sin(2π*ν*x)) + (0.3)^2*(rand()-0.5)\n    ytruth(x) = exp(-x/(0.5)^2) * sin(2π*ν*x)\n\n    X = Tables.table(rand(1,n)', header=[:x])\n    Xtrue = Tables.table(xs', header=[:x])\n\n    y = y.(X.x)\n    ytrue = ytruth.(Xtrue.x)\n\n    return X, y, Xtrue, ytrue\nend\n\nX, y, Xtrue, ytrue = make_training_data(50, 10)\n\n(Tables.MatrixTable{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}} with 50 rows, 1 columns, and schema:\n :x  Float64, [-0.11910037399566864, 0.01220942201251572, -0.010091656163291975, 0.02066646335009669, -0.011549536452583477, 0.8137673436519305, -0.039571239673889064, -0.011172731423395085, -0.144878198731118, -0.12315261399469685  …  -0.033534805230887144, 0.14887469603148903, 0.890109273286779, -0.04974330747314241, 0.2781718602943487, -0.042666066500645396, -0.12454136396033408, -0.06221610591174015, 0.1735472693012275, 0.0501742130486802], Tables.MatrixTable{LinearAlgebra.Adjoint{Float64, Matrix{Float64}}} with 500 rows, 1 columns, and schema:\n :x  Float64, [0.0, 0.12458042096548765, 0.24521471376453977, 0.3600631014574817, 0.46740631751315176, 0.5656708039301793, 0.6534512106157103, 0.7295298841027695, 0.7928930861297541, 0.8427437383060459  …  -0.017831231548023147, -0.016509647442768692, -0.01494870660243864, -0.013176833587402128, -0.011225327522356849, -0.009127822756544184, -0.006919721272661151, -0.004637606245326371, -0.002318646297738392, -4.4860377078854197e-17])\n\n\n\nkernel(θ) = θ.σ₁²*(SqExponentialKernel() ∘ ScaleTransform(1/(2(θ.ℓ₁)^2))) + θ.σ₂²*(Matern32Kernel() ∘ ScaleTransform(1/(2(θ.ℓ₂)^2)))\nθ_init = (;σ₁²=positive(0.1), ℓ₁=positive(0.1), σ₂²=positive(0.1), ℓ₂=positive(0.1),)\ngpr = GPR(\n    k = kernel,\n    θ_init = θ_init, \n    σ²=1e-7\n)\n# mach = machine(gpr, Xtrain, ytrain) |> fit!\nmach = machine(gpr, X, y) |> fit!\n\n[ Info: Training machine(GPR(μ = 0.0, …), …).\n\n\nIter     Function value   Gradient norm \n     0     6.176849e+02     3.294959e+03\n * time: 0.0001609325408935547\n     1     4.245610e+02     2.500000e+01\n * time: 0.01802802085876465\n     2     1.536238e-01     9.276824e+00\n * time: 0.023598909378051758\n     3    -1.838432e+00     1.642444e+01\n * time: 0.028696060180664062\n     4    -2.222085e+00     3.646697e+00\n * time: 0.031058073043823242\n     5    -2.409770e+00     1.895024e+00\n * time: 0.03342390060424805\n     6    -2.424902e+00     4.591384e-02\n * time: 0.03683209419250488\n     7    -2.424945e+00     6.948281e-03\n * time: 0.04055190086364746\n     8    -2.424955e+00     4.642882e-02\n * time: 0.04716300964355469\n     9    -3.911795e+00     1.691929e+01\n * time: 0.06146502494812012\n    10    -3.911854e+00     1.693188e+01\n * time: 0.06484293937683105\n    11    -2.544498e+01     1.811944e+01\n * time: 0.07523202896118164\n    12    -2.773278e+01     1.382678e+01\n * time: 0.08808112144470215\n    13    -3.026212e+01     4.678491e+00\n * time: 0.09667491912841797\n    14    -3.213040e+01     5.934352e+00\n * time: 0.09993100166320801\n    15    -3.418021e+01     5.000502e+00\n * time: 0.10272097587585449\n    16    -3.484923e+01     1.268289e+01\n * time: 0.10554909706115723\n    17    -3.518251e+01     4.125281e+00\n * time: 0.10827898979187012\n    18    -3.540297e+01     2.758169e-01\n * time: 0.11300206184387207\n    19    -3.540563e+01     1.548509e-01\n * time: 0.11755704879760742\n    20    -3.540565e+01     1.663166e-03\n * time: 0.12055611610412598\n    21    -3.540565e+01     9.652292e-06\n * time: 0.12480306625366211\n    22    -3.540565e+01     5.053648e-08\n * time: 0.12941789627075195\n    23    -3.540565e+01     6.864558e-11\n * time: 0.1333160400390625\n\n\ntrained Machine; caches model-specific representations of data\n  model: GPR(μ = 0.0, …)\n  args: \n    1:  Source @492 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @084 ⏎ AbstractVector{Continuous}\n\n\n\ny_pred = predict_mean(mach, Xtrue)\n\n500-element Vector{Float64}:\n  0.5608180743428964\n  0.6071206663803306\n  0.6529973819201168\n  0.6977092492488189\n  0.7404547349060291\n  0.7803823497474104\n  0.8166060889345779\n  0.8482234990255315\n  0.8743360494648164\n  0.8940713721610013\n  0.9066068265312588\n  0.9111937534571365\n  0.9071817048894731\n  ⋮\n -0.06438038355570742\n -0.06776164321695946\n -0.07041055912915144\n -0.0723032111728047\n -0.07343523548509083\n -0.07382060128312681\n -0.07348987474542858\n -0.07248807840635572\n -0.0708722612638966\n -0.06870889576551893\n -0.06607121353515381\n -0.0630365828885893\n\n\n\nplot(Xtrue.x, y_pred)\nscatter!(X.x, y)\n\n\n\n\n\nn=1000\nxs = (rand(n,2) .- 0.5)\nx₁ = xs[:,1]\nx₂ = xs[:,2]\ny = x₁.^5 .+ x₂.^4 .- x₁.^4 .- x₂.^3 \n\nXtrain = Tables.table(xs[1:Int(0.9*n), :], header=[:x₁, :x₂])\nXtest = Tables.table(xs[Int(0.9*n)+1:end, :], header=[:x₁, :x₂])\n\nytrain = y[1:Int(0.9*n)]\nytest = y[Int(0.9*n)+1:end]\n\nscatter(Xtrain.x₁, Xtrain.x₂, ytrain, ms=3, label=\"training points\")\nscatter!(Xtest.x₁, Xtest.x₂, ytest, color=:red, ms=3, label=\"testing points\")\n\n\n\n\n\nkernel(θ) = θ.σ₁²*(SqExponentialKernel() ∘ ScaleTransform(1/(2(θ.ℓ₁)^2))) + θ.σ₂²*(Matern32Kernel() ∘ ScaleTransform(1/(2(θ.ℓ₂)^2)))\nθ_init = (;σ₁²=positive(0.1), ℓ₁=positive(0.1), σ₂²=positive(0.1), ℓ₂=positive(0.1),)\ngpr = GPR(\n    k = kernel,\n    θ_init = θ_init, \n#    σ²=1e-7\n)\n# mach = machine(gpr, Xtrain, ytrain) |> fit!\nmach = machine(gpr, Xtrain, ytrain) |> fit!\n\n[ Info: Training machine(GPR(μ = 0.0, …), …).\n\n\nIter     Function value   Gradient norm \n     0    -1.670727e+02     5.199854e+02\n * time: 0.00019097328186035156\n     1    -4.734137e+03     1.503170e+03\n * time: 4.420762062072754\n     2    -4.840642e+03     9.308025e+02\n * time: 5.104015111923218\n     3    -5.098898e+03     8.038598e+02\n * time: 5.796343088150024\n     4    -5.346684e+03     3.970594e+02\n * time: 6.482567071914673\n     5    -5.896281e+03     1.717814e+03\n * time: 7.4011549949646\n     6    -6.727720e+03     9.644671e+02\n * time: 8.08535099029541\n     7    -6.864112e+03     4.600626e+01\n * time: 8.542866945266724\n     8    -6.886448e+03     4.059468e+02\n * time: 9.247071981430054\n     9    -6.898714e+03     7.109971e+01\n * time: 9.94740605354309\n    10    -6.906918e+03     2.015111e+02\n * time: 10.866995096206665\n    11    -6.913086e+03     1.368285e+02\n * time: 11.548546075820923\n    12    -6.950506e+03     2.913849e+02\n * time: 12.473842144012451\n    13    -7.001777e+03     4.041452e+02\n * time: 13.156044960021973\n    14    -7.046138e+03     1.589057e+02\n * time: 13.838634014129639\n    15    -7.050020e+03     3.377373e+01\n * time: 14.578689098358154\n    16    -7.050116e+03     1.424288e+01\n * time: 15.060181140899658\n    17    -7.050119e+03     8.910666e-01\n * time: 16.643229007720947\n    18    -7.050112e+03     8.341030e-01\n * time: 20.30194902420044\n    19    -7.050112e+03     1.179394e+00\n * time: 26.873029947280884\n    20    -7.050107e+03     2.248200e-01\n * time: 27.352442979812622\n    21    -7.050115e+03     1.044133e+00\n * time: 27.810850143432617\n    22    -7.050117e+03     4.746199e-01\n * time: 32.38437104225159\n    23    -7.050121e+03     8.571094e-01\n * time: 34.8969349861145\n    24    -7.050125e+03     2.832218e-01\n * time: 36.723212003707886\n\n\ntrained Machine; caches model-specific representations of data\n  model: GPR(μ = 0.0, …)\n  args: \n    1:  Source @728 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @679 ⏎ AbstractVector{Continuous}\n\n\n\ny_pred_train = predict_mean(mach, Xtrain)\ny_pred_test = predict_mean(mach, Xtest)\n\n100-element Vector{Float64}:\n -2.8878450393676758e-5\n  0.0344734862446785\n  0.09831034205853939\n -0.01759042777121067\n -0.06359002739191055\n -0.01516430452466011\n  0.0030932649970054626\n -0.0002354402095079422\n -0.014944754540920258\n -0.05883212573826313\n -0.08973219990730286\n -0.00040311552584171295\n -0.01130409725010395\n  ⋮\n -1.8864870071411133e-5\n  0.0848623551428318\n -0.014372158795595169\n  0.001810908317565918\n -0.017009228467941284\n -0.0011161155998706818\n -0.03888287954032421\n -0.020225761458277702\n  0.0007748045027256012\n  0.0014734435826539993\n -0.04117823205888271\n  0.07993144396459684\n\n\n\nscatterresult(\n    ytrain, y_pred_train,\n    ytest, y_pred_test,\n    xlabel=\"Truth\",\n    ylabel=\"Fit\",\n    plot_title=\"Fit\"\n)\n\n\n\n\nOkay, so this confirms that my GPR algorithm"
  }
]